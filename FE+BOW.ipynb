{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXYxEK_ecsoJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_cv(model, features, labels, cv_splits=5):\n",
        "    \"\"\"\n",
        "    Perform cross-validation and return average accuracy, precision, recall, and F1-score.\n",
        "    \"\"\"\n",
        "    scoring = {\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
        "        'recall': make_scorer(recall_score, average='weighted', zero_division=0),\n",
        "        'f1': make_scorer(f1_score, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
        "    scores = {metric: cross_val_score(model, features, labels, cv=skf, scoring=scorer).mean() for metric, scorer in scoring.items()}\n",
        "    return scores\n",
        "\n"
      ],
      "metadata": {
        "id": "BRCiOks-c56-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the dataset file exists\n",
        "file_path = \"English-40TOPICS.csv\"\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"The dataset file '{file_path}' was not found. Please ensure it is in the correct directory.\")\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Assuming the last column contains labels\n",
        "features = data.iloc[:, :-1].values\n",
        "labels = data.iloc[:, -1].values\n",
        "\n"
      ],
      "metadata": {
        "id": "2_57upinc9ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure labels are categorical\n",
        "if labels.dtype.kind in {'f', 'u'}:  # Check if labels are float or unsigned int\n",
        "    labels = labels.astype(int)  # Convert to integer if numeric\n",
        "elif labels.dtype.kind == 'O':  # Check if labels are object (e.g., strings)\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    encoder = LabelEncoder()\n",
        "    labels = encoder.fit_transform(labels)\n",
        "\n",
        "# Apply Min-Max Scaling to ensure non-negative values for LDA\n",
        "scaler = MinMaxScaler()\n",
        "features = scaler.fit_transform(features)\n"
      ],
      "metadata": {
        "id": "aG6EeeocdDgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LDA for feature extraction\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)  # Adjust n_components as needed\n",
        "features_lda = lda.fit_transform(features)\n",
        "\n",
        "# Add variance explanation for LDA\n",
        "explained_variance = lda.components_.var(axis=1)\n",
        "print(\"\\nExplained Variance by LDA Components:\")\n",
        "for i, var in enumerate(explained_variance):\n",
        "    print(f\"Topic {i + 1}: {var:.4f}\")\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n"
      ],
      "metadata": {
        "id": "D0byCDGtdH-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation for all classifiers\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    results[name] = evaluate_model_cv(clf, features_lda, labels)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nClassification Results:\")\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for metric, score in metrics.items():\n",
        "        print(f\"  {metric.capitalize()}: {score:.4f}\")\n",
        "\n",
        "# Compare feature reduction before and after LDA\n",
        "print(\"\\nFeature Reduction:\")\n",
        "print(f\"Original Feature Count: {features.shape[1]}\")\n",
        "print(f\"Reduced Feature Count: {features_lda.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "NeCQq0IIdMvs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}